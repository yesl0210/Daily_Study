{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     level    lang tweets  phd  interview\n",
      "0   senior    java     no   no      False\n",
      "25     mid  python    yes  yes       True\n",
      "28     mid    java     no   no      False\n",
      "22  junior       R    yes  yes       True\n",
      "6      mid       R    yes  yes       True\n",
      "9   junior  python    yes   no       True\n",
      "18  senior    java     no  yes       True\n",
      "15  senior       R    yes  yes       True\n",
      "8   senior       R    yes   no       True\n",
      "17  junior    java    yes  yes      False\n",
      "14  senior    java    yes  yes      False\n",
      "26     mid    java    yes  yes      False\n",
      "24  senior    java     no   no      False\n",
      "7   senior  python     no   no      False\n",
      "20     mid  python    yes  yes      False\n",
      "3   junior  python     no   no       True\n",
      "13  junior  python     no  yes      False\n",
      "23     mid    java     no   no       True\n",
      "10  senior  python    yes  yes       True\n",
      "16     mid    java    yes   no       True\n",
      "29     mid  python     no  yes      False\n",
      "2      mid  python     no   no       True\n",
      "4   junior       R    yes   no       True\n",
      "19     mid       R    yes   no       True\n",
      "21     mid  python     no   no       True\n",
      "11     mid  python     no  yes       True\n",
      "1   senior    java     no  yes      False\n",
      "****************************************************TEST\n",
      "     level  lang tweets  phd  interview\n",
      "5   junior     R    yes  yes      False\n",
      "27  senior     R    yes  yes       True\n",
      "12     mid  java    yes   no       True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "data = pd.read_excel('DecisionData.xlsx','Sheet1')\n",
    "data = data[[\"District\",\"House Type\",\"Incom\",\"Previous Customer\",\"Outcome\"]]\n",
    "\n",
    "# attribute\n",
    "features = data[[\"District\", \"House Type\",\"Incom\",\"Previous Customer\"]]\n",
    "# Label for decision\n",
    "target = data[\"Outcome\"]\n",
    "'''\n",
    "\n",
    "data = pd.read_csv('decision_tree_data.csv',encoding='utf-8')\n",
    "\n",
    "# split data set(30) into training set(27), test set(3) randomly\n",
    "# data : original data we want to split\n",
    "# test_size : size of test data set\n",
    "# random_state = used to split data randomly as random seed\n",
    "train, test= train_test_split(data, test_size=0.1,random_state=random.randint(1,100))\n",
    "\n",
    "# attribute\n",
    "features = train[[\"level\", \"lang\",\"tweets\",\"phd\"]]\n",
    "# Label for decision\n",
    "target = train[\"interview\"]\n",
    "\n",
    "# attribute\n",
    "features_test = test[[\"level\", \"lang\",\"tweets\",\"phd\"]]\n",
    "# Label for decision\n",
    "target_test = test[\"interview\"]\n",
    "\n",
    "print(train) # 27\n",
    "print(\"****************************************************TEST\")\n",
    "print(test) # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate entropy about each node\n",
    "# *Input*\n",
    "# target_col : node to calculate\n",
    "# *Output*\n",
    "# return entropy : Return the calculated entropy value.\n",
    "def entropy(target) :\n",
    "    # find the number of'Respond', 'Nothing' in input attribute\n",
    "    element, count = np.unique(target,return_counts=True) \n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum([(count[i]/np.sum(count))*np.log(count[i]/np.sum(count)) for i in range(len(element))])\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "# Calculate information gain to set the node on which to classify\n",
    "# *Input*\n",
    "# data : original data\n",
    "# split_attribute_name : child node(branch) of parent node.\n",
    "# -> If it have a lot of information gain, it can be a classification criterion.\n",
    "# target_name : mean 'Label' attribute name - \"Outcome\"\n",
    "# *Output*\n",
    "# return Information_Gain : it will be used to find largest information gain and select next node\n",
    "def InfoGain(data,split_attribute_name,target_name) :\n",
    "    \n",
    "    # calculate total entropy\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    # Calculate sum of entropy of child nodes.\n",
    "    # The weights are taken into account and calculated.\n",
    "    vals, counts = np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    # 선택된 attribute의 자식들 = unique 한 값들의 weight 을 고려하여 entroypy 계산\n",
    "    Weighted_Entropy = np.sum([counts[i]*\n",
    "                               entropy(data.where(data[split_attribute_name]\n",
    "                                                  ==vals[i]).dropna()[target_name])\n",
    "                               for i in range(len(vals))])/np.sum(counts)\n",
    "    \n",
    "    # Calculate information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall tree by selecting some attribute as classification criteria\n",
    "# use entropy, information gain function\n",
    "# *input *\n",
    "# data : Data other than attribution completed with calculation and tree configuration\n",
    "# originaldata : original data\n",
    "# features : attribution used for decision making except label\n",
    "# target_attribute_name : Lable - \"Outcome\"\n",
    "# * Output *\n",
    "#  return structure of tree\n",
    "def ID3(data,originaldata,features,target_attribute_name,parent_node_class = None):\n",
    " \n",
    "    # Define the criteria that are stopped.\n",
    " \n",
    "    # 1.If the target property has a single value: return the target property\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    " \n",
    "    # 2. Data missing: Returns the target property with the maximum value from the source data\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])\\\n",
    "               [np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])]\n",
    " \n",
    "    # 3. When no technical properties exist: return destination properties of the parent node\n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    " \n",
    "    # Perform structuralization, such as adding nodes and attribution to the tree.\n",
    "    else:\n",
    "        # Define destination attribute for parent node such as \"Outcom\"\n",
    "        parent_node_class = np.unique(data[target_attribute_name])\\\n",
    "                            [np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "        \n",
    "        # Select attribute to split data\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] # get informatinGain to all attributes\n",
    "        best_feature_index = np.argmax(item_values) # find attribute that has largest informatin gain\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        \n",
    "       # InfoGainResult(data,best_feature,target_attribute_name) # print entropy about child node of selected attribute\n",
    "       # print(best_feature,\"information gain : \",round(np.max(item_values),5)) # print information gain about selected attribute\n",
    "        \n",
    "        # create tree\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        # except attribute that has largest information gain\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        # create and select branch node\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            # data partitioning. delete data have attribution completed with calculation and tree configuration\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            # Use recursive function to set classification criteria for newly formed branches\n",
    "            subtree = ID3(sub_data,data,features,target_attribute_name,parent_node_class)\n",
    "            tree[best_feature][value] = subtree # Add a branch to an existing tree\n",
    "            \n",
    "                        \n",
    "        return(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, input):\n",
    "    # Input is classified based on the given tree.\n",
    "    \n",
    "    # if it is leaf node, return tree\n",
    "    if tree in [True, False]:\n",
    "        return tree\n",
    "\n",
    "    #attribute, subtree_dict = tree.items()\n",
    "    # if it is not leaf node, will be classify.\n",
    "    # to do this, get the nodes that are the criteria for classification from tree\n",
    "    attribute = list(tree.keys()).pop()\n",
    "    subtree_dict = list(tree.values())[0]\n",
    "\n",
    "    # get attribute of input\n",
    "    subtree_key = input.get(attribute)\n",
    "\n",
    "\n",
    "    # When there is not sub tree about key data of input --> return None\n",
    "    if subtree_key not in subtree_dict:\n",
    "        subtree_key = None \n",
    "        return None\n",
    "\n",
    "    # select sub tree for key of input data\n",
    "    subtree = subtree_dict[subtree_key]\n",
    "    \n",
    "    # classify input data and  recursive function for classify about sub-tree\n",
    "    return classify(subtree, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Structure of decision tree *\n",
      "{'lang': {'R': 1.0,\n",
      "          'java': {'level': {'junior': 0.0,\n",
      "                             'mid': {'phd': {'no': {'tweets': {'no': 1.0,\n",
      "                                                               'yes': 1.0}},\n",
      "                                             'yes': 0.0}},\n",
      "                             'senior': {'phd': {'no': 0.0,\n",
      "                                                'yes': {'tweets': {'no': 0.0,\n",
      "                                                                   'yes': 0.0}}}}}},\n",
      "          'python': {'phd': {'no': {'level': {'junior': 1.0,\n",
      "                                              'mid': 1.0,\n",
      "                                              'senior': 0.0}},\n",
      "                             'yes': {'level': {'junior': 0.0,\n",
      "                                               'mid': {'tweets': {'no': 0.0,\n",
      "                                                                  'yes': 0.0}},\n",
      "                                               'senior': 1.0}}}}}}\n",
      "****Classify****\n",
      "level        junior\n",
      "lang              R\n",
      "tweets          yes\n",
      "phd             yes\n",
      "interview     False\n",
      "Name: 5, dtype: object\n",
      "Result: Interview OK!\n",
      "\n",
      "level        senior\n",
      "lang              R\n",
      "tweets          yes\n",
      "phd             yes\n",
      "interview      True\n",
      "Name: 27, dtype: object\n",
      "Result: Interview OK!\n",
      "\n",
      "level         mid\n",
      "lang         java\n",
      "tweets        yes\n",
      "phd            no\n",
      "interview    True\n",
      "Name: 12, dtype: object\n",
      "Result: Interview OK!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree = ID3(train, train, [\"level\", \"lang\",\"tweets\",\"phd\"],\"interview\")\n",
    "from pprint import pprint\n",
    "print(\"* Structure of decision tree *\")\n",
    "pprint(tree)\n",
    "\n",
    "print(\"****Classify****\")\n",
    "\n",
    "for i in range(3) :\n",
    "    result = classify(tree,test.iloc[i])\n",
    "    print(test.iloc[i])\n",
    "    if result == 1 :\n",
    "        print(\"Result: Interview OK!\\n\")\n",
    "    elif result == 0 :\n",
    "        print(\"Result: Interview NO!\\n\")\n",
    "    else :\n",
    "        print(\"Result: None\\n\")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
