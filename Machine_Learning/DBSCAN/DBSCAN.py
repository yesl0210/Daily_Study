
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt


# In[2]:


df = pd.read_csv('mushrooms.csv') # 8124개
df.head(10)
#print(df.describe())
#print(df.isna().sum()) # Missing Value 없음


# In[3]:


X = df.drop(['class'], axis = 1)
y = df['class']


# In[4]:


# Label Encoding - class
labelEncoder = LabelEncoder()
y = labelEncoder.fit_transform(y)

# Label Encoding - predictor
for col in X.columns : 
    X[col] = labelEncoder.fit_transform(X[col])


# In[5]:


# Scaling - 둘 중 하나

# MinMaxScaler
# mScaler = MinMaxScaler()
# X = mScaler.fit_transform(X)

# # Standard Scaler
# sScaler = StandardScaler()
# X = sScaler.fit_transform(X)


# In[ ]:


# 그래프로 클러스터링 표현하기

import random

# parameter - labels : label obtained after clustering
# Output - color list Generated by the number of labels
def color(labels) :
    unique_labels = set(labels) # get unique label 
    # slice color by considerig the number of unique labels
    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))] 
    colors.append([0,0,0,1]) # add black color for noise
    return colors


# In[6]:


# DBSCAN Clustering
from collections import Counter
from sklearn.metrics.cluster import normalized_mutual_info_score

eps=[0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5]
min_samples= [3, 5, 10, 15, 20, 30, 50, 100]
distance = ['euclidean', 'hamming']
p = [0, 1, 2, 3, 4, 5]
purityList = [] # purity list for results of each parameter combinations
maxPurity = 0;
bestParam = ['','','', ''];

# Preprocessing 
for prepro in range(3) :            
    # Label Encoding을 한 경우에만, Scaling
    if(prepro == 1) : # Do MinMax Scaler
        mScaler = MinMaxScaler()
        X = mScaler.fit_transform(X)
    if(prepro == 2) : # Do Standard Scaler
        sScaler = StandardScaler()
        X = sScaler.fit_transform(X)
    print("Preprocess option: ",prepro)
    # p 추가해야함
    for epsP in eps :
        for minP in min_samples :
            for distanceP in distance :
                print("epsP: ",epsP, ", min_samples: ",minP, ", distance :",distanceP)
                dbscan = DBSCAN(eps = epsP, min_samples = minP, metric=distanceP).fit(X)
                nCluster = len(Counter(dbscan.labels_)) # 만들어진 cluster의 수 - 1 (noise cluster)
                size =len(dbscan.labels_)
                #print(Counter(dbscan.labels_))
                if(-1 in Counter(dbscan.labels_)) :
                    nCluster -= 1
                class_count = np.zeros((nCluster,2)) # 열이 2개 0,1 로 & 만들어진 클러스터 수 만큼
                noise_count = np.zeros((1,2)) # noise를 위한 count
                print("dbscan.labels count: ",len(Counter(dbscan.labels_))) # label이 -1이 없는 경우도 있음..!!!! <- 처리해줘야함
                print("** dbscan.labels_ **")
                print(Counter(dbscan.labels_))
                print("nCluster: ",nCluster)
                print("class_count: ",len(class_count))
                print("\n\n")
                for index in range(size) : # 모든 데이터에 대해서 다 함
                    label = dbscan.labels_[index] # 현재 인덱스의 label
                    if (label == -1) : # label -1이면(노이즈) 0으로 취급
                        noise_count[0][y[index]] += 1 # 첫번째 - label, 두번째 - 실제 class(0 혹은 1)
                    else : # label이 -1이 아닐때(noise가 )
                        class_count[label][y[index]] += 1 # 수 증가 <-- 여기가 문제

                sum_majority = np.zeros((nCluster,1))
                for index in range(nCluster) : # 모든 클러스터에 대해서
                    sum_majority[index] = max(class_count[index][0], class_count[index][1]) # 1과 0 중, 더 큰 class의 개수로 할당

                purity = (np.sum(sum_majority)+max(noise_count[0][0],noise_count[0][1]))/size
                purityList.append(purity)
                if(maxPurity <= purity) :
                    maxPurity = purity
                    bestParam[0] = epsP
                    bestParam[1] = minP
                    bestParam[2] = distanceP
                    # bestParam = "eps: ", epsP, ", min_samples : ", minP,", metric : ", distanceP
                    if(prepro == 1) :
                        bestParam[3] = 'MinMax Scaler'
                    if(prepro == 2) :
                        bestParam[3] = 'Standard Scaler'
print(purity)
print("Max Purity : ", maxPurity, ", Best Prameter: ", bestParam)


# In[9]:


print(Counter(purityList))
print(len(purityList))

# dbscan 에서 unique만 뽑으면 -> 확인된 클러스터의 집합 = 오메가(강의자료)
# y에서 1,0 -> 실제 class = C (강의자료)
# Cluster의 집합 개수(오메가의 개수) => 1부터 K 까지
# 실제 class의 집합 개수 (C의 개수) => 1부터 J 까지

# 각 class(1부터 J까지 있음) 내에서, 할당되지 않은 클러스터(K-J)로부터
# 가장 인구가 많은 클러스터의 크기를 찾아라
# 그리고 이걸 다 더한 후, N으로 나눠준다.

# 각 cluster는 해당 클러스터안에서 가장 많은 class에 할당되어 있다.
# label이 없는 채로 클러스터링 하기 때문에,
# -> label과는 상관없이 분류되어짐 => class 가 많이 다르고 많거나 적을 수 있음.


# 그러면!
# 클러스터 별로 해야함 -> dbscan.labels_ 에 클러스터 할당되어 있음
# 각 클러스터에서 실제 label이 Majority인 class를 뽑음 -> 개수도 같이 뽑음
# 각 클러스터에서 구한 모든 Majority 를 더함
# Purity = (1/데이터 크기 = 8124..?) * (모든 Majority 더한 값)

# Q : 각 클러스터 내에서 Majority class를 어떻게 뽑느냐?
# dbscan.labels_의 index 처음부터 끝까지 !
# count0(실제 class가 0인 데이터를 위한)과 count1(실제 class가 1인 데이터를 위한) 라는 ndarray (클러스터 개수만큼) 선언.
# -> 0으로 모두 초기화 --> np.zero((3,4)) = 3,4크기의 어레이 만드는데 모두 0으로 초기화되어있음
# index 0(첫번째) 데이터의 label이 -1로 할당됨 & 실제 class는 0
# -> label -1은 0으로 취급
# -> 실제 class가 0 이므로 count0 어레이의 첫번째 인덱스로 들어감 -> count0[0] += 1 (1을 더해줌)
# 즉, count0과 count1 어레이 모두 각 클러스터에 대해서 있는 것.
# 다 ~ 끝나면, 각 클러스터별로 Majority class 비교.
# count0[10]과 count1[10] 중 더 값이(size) 큰 걸 찾음 (10 = 클러스터 label이 10으로 된 것)
# 만약, count0[10]이 더 크다면, 총합할때 count0[10]의 값이 들어감.
# 더하는건 클러스터 갯수만큼!
# **** 단, 계산할때 label이 0이나 1로 결정된 클러스터들은 제외함!
# 모든 class에 대해서



# dbscan.labels_ 보고 index 뽑기 -> 해당 label에 넣기
# label이 -1 이면, 


# nmi = normalized_mutual_info_score(y, dbscan.labels_);
# print(nmi)

# purity = (1/size) * 



# purity = np.count_nonzero(cluster==count)/np.size(cluster)
# print(purity)


# for eps_i in eps : # tuning eps parameter
#     for min_j in min_samples : # tuning min_samples parameter
        
#         # Create DBSCAN Model by using parameter (eps, min_samples)
#         dbscan = DBSCAN(eps = eps_i, min_samples = min_j, metric='euclidean').fit(X)
#         purity = np.count_nonzero(cluster==count)/np.size(cluster)
#         purity_list.append(purity)

        
        
        # 일단 그림은 빼고 생각
#         # result for labels after DBSCAN
#         db_labels = db_model.labels_ 
        
#         # Create color list by considering the number of labels
#         colors = color(db_labels)
    
#         # Building the colour vector for each data point 
#         # db_cvec = [colours[label] for label in db_labels] 
#         db_cvec = [colors[label] for label in db_labels] 
        
#         # Plotting P1 on the X-Axis and P2 on the Y-Axis 
#         # according to the colour vector defined 
#         plt.figure(figsize =(9, 9)) 
#         plt.scatter(X[:,0],X[:,1], c = db_cvec)

